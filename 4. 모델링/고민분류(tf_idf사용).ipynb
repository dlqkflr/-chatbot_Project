{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('고민분류용df2.csv', encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "17315    None\n",
       "17316    None\n",
       "17317    None\n",
       "17318    None\n",
       "17319    None\n",
       "Name: text, Length: 17312, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "df.text.apply(lambda x : data.append(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=1000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = tfidf.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17312, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data2, df['label'], test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15588, 1000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15588, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation ='relu', input_shape=(1000,)))\n",
    "model.add(layers.Dense(256, activation= 'relu'))\n",
    "model.add(layers.Dense(256, activation= 'relu'))\n",
    "model.add(layers.Dense(64, activation= 'relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\\\n",
    "              loss='categorical_crossentropy',\\\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12470 samples, validate on 3118 samples\n",
      "Epoch 1/10\n",
      "12470/12470 [==============================] - ETA: 32s - loss: 2.3103 - acc: 0.09 - ETA: 7s - loss: 2.2927 - acc: 0.2406 - ETA: 4s - loss: 2.2643 - acc: 0.280 - ETA: 3s - loss: 2.2191 - acc: 0.289 - ETA: 2s - loss: 2.1466 - acc: 0.337 - ETA: 2s - loss: 2.0455 - acc: 0.386 - ETA: 1s - loss: 1.9363 - acc: 0.432 - ETA: 1s - loss: 1.8123 - acc: 0.476 - ETA: 1s - loss: 1.6934 - acc: 0.509 - ETA: 1s - loss: 1.5793 - acc: 0.544 - ETA: 1s - loss: 1.4747 - acc: 0.574 - ETA: 1s - loss: 1.3735 - acc: 0.603 - ETA: 0s - loss: 1.2882 - acc: 0.627 - ETA: 0s - loss: 1.2098 - acc: 0.651 - ETA: 0s - loss: 1.1439 - acc: 0.671 - ETA: 0s - loss: 1.0860 - acc: 0.687 - ETA: 0s - loss: 1.0343 - acc: 0.702 - ETA: 0s - loss: 0.9943 - acc: 0.714 - ETA: 0s - loss: 0.9515 - acc: 0.726 - ETA: 0s - loss: 0.9117 - acc: 0.737 - ETA: 0s - loss: 0.8749 - acc: 0.748 - ETA: 0s - loss: 0.8455 - acc: 0.757 - ETA: 0s - loss: 0.8185 - acc: 0.765 - ETA: 0s - loss: 0.7930 - acc: 0.773 - ETA: 0s - loss: 0.7750 - acc: 0.778 - 2s 145us/step - loss: 0.7688 - acc: 0.7805 - val_loss: 0.1876 - val_acc: 0.9484\n",
      "Epoch 2/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.1217 - acc: 0.960 - ETA: 1s - loss: 0.1348 - acc: 0.967 - ETA: 1s - loss: 0.1392 - acc: 0.967 - ETA: 1s - loss: 0.1278 - acc: 0.968 - ETA: 1s - loss: 0.1295 - acc: 0.966 - ETA: 1s - loss: 0.1259 - acc: 0.966 - ETA: 1s - loss: 0.1262 - acc: 0.965 - ETA: 0s - loss: 0.1225 - acc: 0.966 - ETA: 0s - loss: 0.1208 - acc: 0.967 - ETA: 0s - loss: 0.1194 - acc: 0.966 - ETA: 0s - loss: 0.1179 - acc: 0.966 - ETA: 0s - loss: 0.1150 - acc: 0.967 - ETA: 0s - loss: 0.1164 - acc: 0.966 - ETA: 0s - loss: 0.1158 - acc: 0.967 - ETA: 0s - loss: 0.1160 - acc: 0.966 - ETA: 0s - loss: 0.1158 - acc: 0.967 - ETA: 0s - loss: 0.1162 - acc: 0.966 - ETA: 0s - loss: 0.1160 - acc: 0.966 - ETA: 0s - loss: 0.1177 - acc: 0.966 - ETA: 0s - loss: 0.1205 - acc: 0.965 - ETA: 0s - loss: 0.1202 - acc: 0.965 - ETA: 0s - loss: 0.1215 - acc: 0.965 - ETA: 0s - loss: 0.1225 - acc: 0.965 - ETA: 0s - loss: 0.1220 - acc: 0.964 - ETA: 0s - loss: 0.1206 - acc: 0.965 - 1s 118us/step - loss: 0.1210 - acc: 0.9654 - val_loss: 0.1635 - val_acc: 0.9580\n",
      "Epoch 3/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0278 - acc: 0.992 - ETA: 1s - loss: 0.0437 - acc: 0.992 - ETA: 1s - loss: 0.0485 - acc: 0.992 - ETA: 1s - loss: 0.0495 - acc: 0.989 - ETA: 1s - loss: 0.0523 - acc: 0.989 - ETA: 1s - loss: 0.0497 - acc: 0.990 - ETA: 1s - loss: 0.0487 - acc: 0.989 - ETA: 0s - loss: 0.0485 - acc: 0.989 - ETA: 0s - loss: 0.0481 - acc: 0.989 - ETA: 0s - loss: 0.0510 - acc: 0.988 - ETA: 0s - loss: 0.0504 - acc: 0.988 - ETA: 0s - loss: 0.0520 - acc: 0.987 - ETA: 0s - loss: 0.0522 - acc: 0.987 - ETA: 0s - loss: 0.0535 - acc: 0.986 - ETA: 0s - loss: 0.0542 - acc: 0.985 - ETA: 0s - loss: 0.0581 - acc: 0.984 - ETA: 0s - loss: 0.0592 - acc: 0.984 - ETA: 0s - loss: 0.0610 - acc: 0.984 - ETA: 0s - loss: 0.0596 - acc: 0.984 - ETA: 0s - loss: 0.0612 - acc: 0.984 - ETA: 0s - loss: 0.0621 - acc: 0.983 - ETA: 0s - loss: 0.0651 - acc: 0.982 - ETA: 0s - loss: 0.0660 - acc: 0.982 - ETA: 0s - loss: 0.0661 - acc: 0.982 - ETA: 0s - loss: 0.0657 - acc: 0.982 - 1s 118us/step - loss: 0.0665 - acc: 0.9822 - val_loss: 0.1623 - val_acc: 0.9602\n",
      "Epoch 4/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0131 - acc: 1.000 - ETA: 1s - loss: 0.0427 - acc: 0.992 - ETA: 1s - loss: 0.0367 - acc: 0.993 - ETA: 1s - loss: 0.0389 - acc: 0.993 - ETA: 1s - loss: 0.0343 - acc: 0.993 - ETA: 1s - loss: 0.0332 - acc: 0.993 - ETA: 1s - loss: 0.0346 - acc: 0.991 - ETA: 0s - loss: 0.0340 - acc: 0.991 - ETA: 0s - loss: 0.0329 - acc: 0.991 - ETA: 0s - loss: 0.0337 - acc: 0.992 - ETA: 0s - loss: 0.0348 - acc: 0.991 - ETA: 0s - loss: 0.0352 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.990 - ETA: 0s - loss: 0.0411 - acc: 0.989 - ETA: 0s - loss: 0.0403 - acc: 0.989 - ETA: 0s - loss: 0.0409 - acc: 0.989 - ETA: 0s - loss: 0.0403 - acc: 0.989 - ETA: 0s - loss: 0.0407 - acc: 0.989 - ETA: 0s - loss: 0.0423 - acc: 0.989 - ETA: 0s - loss: 0.0433 - acc: 0.988 - ETA: 0s - loss: 0.0428 - acc: 0.988 - ETA: 0s - loss: 0.0435 - acc: 0.988 - ETA: 0s - loss: 0.0427 - acc: 0.988 - ETA: 0s - loss: 0.0438 - acc: 0.987 - ETA: 0s - loss: 0.0437 - acc: 0.987 - 1s 119us/step - loss: 0.0438 - acc: 0.9877 - val_loss: 0.1869 - val_acc: 0.9538\n",
      "Epoch 5/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0272 - acc: 1.000 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0261 - acc: 0.990 - ETA: 1s - loss: 0.0290 - acc: 0.989 - ETA: 1s - loss: 0.0281 - acc: 0.990 - ETA: 1s - loss: 0.0311 - acc: 0.990 - ETA: 1s - loss: 0.0311 - acc: 0.990 - ETA: 1s - loss: 0.0289 - acc: 0.991 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0273 - acc: 0.991 - ETA: 0s - loss: 0.0268 - acc: 0.991 - ETA: 0s - loss: 0.0300 - acc: 0.991 - ETA: 0s - loss: 0.0316 - acc: 0.990 - ETA: 0s - loss: 0.0300 - acc: 0.991 - ETA: 0s - loss: 0.0314 - acc: 0.990 - ETA: 0s - loss: 0.0309 - acc: 0.990 - ETA: 0s - loss: 0.0308 - acc: 0.990 - ETA: 0s - loss: 0.0308 - acc: 0.990 - ETA: 0s - loss: 0.0318 - acc: 0.990 - ETA: 0s - loss: 0.0324 - acc: 0.990 - ETA: 0s - loss: 0.0327 - acc: 0.990 - ETA: 0s - loss: 0.0323 - acc: 0.990 - ETA: 0s - loss: 0.0318 - acc: 0.990 - ETA: 0s - loss: 0.0321 - acc: 0.990 - ETA: 0s - loss: 0.0328 - acc: 0.990 - 1s 116us/step - loss: 0.0327 - acc: 0.9906 - val_loss: 0.2050 - val_acc: 0.9567\n",
      "Epoch 6/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0043 - acc: 1.000 - ETA: 1s - loss: 0.0227 - acc: 0.993 - ETA: 1s - loss: 0.0265 - acc: 0.993 - ETA: 1s - loss: 0.0238 - acc: 0.994 - ETA: 1s - loss: 0.0269 - acc: 0.993 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0266 - acc: 0.993 - ETA: 1s - loss: 0.0282 - acc: 0.993 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0261 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.992 - ETA: 0s - loss: 0.0264 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0273 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0285 - acc: 0.992 - ETA: 0s - loss: 0.0276 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.992 - 1s 119us/step - loss: 0.0266 - acc: 0.9929 - val_loss: 0.2188 - val_acc: 0.9532\n",
      "Epoch 7/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0094 - acc: 1.000 - ETA: 1s - loss: 0.0258 - acc: 0.993 - ETA: 1s - loss: 0.0299 - acc: 0.993 - ETA: 1s - loss: 0.0256 - acc: 0.994 - ETA: 1s - loss: 0.0243 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.995 - ETA: 1s - loss: 0.0205 - acc: 0.995 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.993 - ETA: 0s - loss: 0.0232 - acc: 0.993 - ETA: 0s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0232 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0231 - acc: 0.993 - ETA: 0s - loss: 0.0233 - acc: 0.993 - ETA: 0s - loss: 0.0231 - acc: 0.993 - 1s 120us/step - loss: 0.0231 - acc: 0.9933 - val_loss: 0.2197 - val_acc: 0.9554\n",
      "Epoch 8/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0060 - acc: 1.000 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 1s - loss: 0.0198 - acc: 0.993 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0197 - acc: 0.994 - ETA: 1s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0203 - acc: 0.993 - ETA: 0s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0205 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - 1s 119us/step - loss: 0.0219 - acc: 0.9933 - val_loss: 0.2099 - val_acc: 0.9551\n",
      "Epoch 9/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0486 - acc: 0.984 - ETA: 1s - loss: 0.0335 - acc: 0.989 - ETA: 1s - loss: 0.0227 - acc: 0.993 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - 1s 118us/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.2226 - val_acc: 0.9541\n",
      "Epoch 10/10\n",
      "12470/12470 [==============================] - ETA: 1s - loss: 0.0124 - acc: 1.000 - ETA: 1s - loss: 0.0100 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.994 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.993 - ETA: 0s - loss: 0.0183 - acc: 0.993 - ETA: 0s - loss: 0.0182 - acc: 0.993 - ETA: 0s - loss: 0.0185 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0187 - acc: 0.993 - ETA: 0s - loss: 0.0189 - acc: 0.993 - ETA: 0s - loss: 0.0192 - acc: 0.993 - ETA: 0s - loss: 0.0188 - acc: 0.993 - ETA: 0s - loss: 0.0184 - acc: 0.993 - ETA: 0s - loss: 0.0184 - acc: 0.993 - ETA: 0s - loss: 0.0184 - acc: 0.993 - ETA: 0s - loss: 0.0180 - acc: 0.993 - 1s 116us/step - loss: 0.0180 - acc: 0.9935 - val_loss: 0.2181 - val_acc: 0.9577\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\\\n",
    "                    epochs=10, batch_size = 128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1732/1732 [==============================] - ETA:  - ETA:  - ETA:  - 0s 68us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24146388441444683, 0.9509237875288684]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 운동몸매 :1 , 부부 :2 , 연애 :3 , 외모 : 4 , 진로 :5 ,직장 :6, 이별 7 , 친구 :8, 심리 : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data2, df['label'], test_size = 0.2, random_state = 42)\n",
    "\n",
    "y_train= y_train.values\n",
    "y_test= y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c_pred= clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_pred = forest.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤포레스트정확도 : 0.9754548079699682\n"
     ]
    }
   ],
   "source": [
    "print('랜덤포레스트정확도 :', metrics.accuracy_score(y_test, y_f_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤포레스트정확도 : 0.9719976905311778\n",
      "아다 부스트 정확도 : 0.8279445727482679\n"
     ]
    }
   ],
   "source": [
    "print('랜덤포레스트정확도 :', metrics.accuracy_score(y_test, y_f_pred))\n",
    "print('아다 부스트 정확도 :', metrics.accuracy_score(y_test, y_c_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 랜덤포레스트 정확도 : 97%\n",
    "* Deeplearning 정확도 : 95%\n",
    "* LSTM 정확도 : 87 %\n",
    "* 아다부스트 정확도 : 82 %\n",
    "\n",
    "## 따라서 랜덤포레스트 모델로 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용했던 TfIDf 값과 랜덤 포레스트 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(tfidf, open(\"gomin_classif_tfidf.obj\",\"wb\"))\n",
    "dill.dump(forest, open(\"gomin_classif_RF.obj\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag2(sentences):\n",
    "    # KoNLPy 형태소분석기 설정\n",
    "    tagger = Okt()\n",
    "    \n",
    "    # 문장 품사 변수 초기화\n",
    "    sentences_pos = []\n",
    "    sentence = \" \".join(tagger.morphs(sentences))\n",
    "    sentences_pos.append(sentence)\n",
    "    \n",
    "  \n",
    "    return sentences_pos\n",
    "\n",
    "sentence ='와이프가 요새 저한테 소홀해요'\n",
    "\n",
    "sentence = pos_tag2(sentence)\n",
    "\n",
    "\n",
    "test_sen = tfidf.transform(sentence).toarray()\n",
    "\n",
    "a=forest.predict(test_sen)\n",
    "\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence ='와이프가 요새 저한테 소홀해요'\n",
    "\n",
    "sentence = pos_tag2(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['와이프 가 요새 저 한테 소홀해요']"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sen = tfidf.transform(sentence).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 운동몸매 :1 , 부부 :2 , 연애 :3 , 외모 : 4 , 진로 :5 ,직장 :6, 이별 7 , 친구 :8, 심리 : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "a=forest.predict(test_sen)\n",
    "\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9], dtype=int64)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = dill.load(open(\"RF_gomin_categori.obj\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_test.predict(test_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용결과 랜덤포레스트가 가장 잘 분류함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
